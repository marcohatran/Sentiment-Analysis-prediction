{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport re\nimport nltk\nfrom sklearn.datasets import load_files\nnltk.download('stopwords')\nimport pickle\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nimport pickle\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.externals import joblib\nimport pandas as pd",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv(\"train.csv\", encoding = \"utf-8\")\ndf.head()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>@user when a father is dysfunctional and is s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>@user @user thanks for #lyft credit i can't us...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>bihday your majesty</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>#model   i love u take with u all the time in ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>factsguide: society now    #motivation</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   id  label                                              tweet\n0   1      0   @user when a father is dysfunctional and is s...\n1   2      0  @user @user thanks for #lyft credit i can't us...\n2   3      0                                bihday your majesty\n3   4      0  #model   i love u take with u all the time in ...\n4   5      0             factsguide: society now    #motivation"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X, y = df.tweet, df.label",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.download('wordnet')\ndocuments = []\n\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = WordNetLemmatizer()\n\nfor sen in range(0, len(X)):\n    # Remove all the special characters\n    document = re.sub(r'\\W', ' ', str(X[sen]))\n    \n    # remove all single characters\n    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n    \n    # Remove single characters from the start\n    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n    \n    # Substituting multiple spaces with single space\n    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n    \n    # Removing prefixed 'b'\n    document = re.sub(r'^b\\s+', '', document)\n    \n    # Converting to Lowercase\n    document = document.lower()\n    \n    # Lemmatization\n    document = document.split()\n\n    document = [stemmer.lemmatize(word) for word in document]\n    document = ' '.join(document)\n    \n    documents.append(document)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package wordnet to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfTransformer\nmodel = Pipeline([('vectorizer', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],\n               'tfidf__use_idf': (True, False)}",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)\ngs_clf_svm = gs_clf_svm.fit(X, y)\nprint(gs_clf_svm.best_score_)\nprint(gs_clf_svm.best_params_)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n  warnings.warn(CV_WARNING, FutureWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "0.9626431387272386\n{'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 2)}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n    ('tfidf', TfidfTransformer(use_idf=True)),\n    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.fit(X_train, y_train)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "Pipeline(memory=None,\n     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n       ..._class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0),\n          n_jobs=None))])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred = model.predict(X_test)",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "accuracy_score(y_test, pred)\n",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "0.9599562020960426"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "joblib.dump(model, 'model.pkl', compress=1)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "['model.pkl']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}